{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final project: Basics of Mobile Robotics**\n",
    "\n",
    "Members of the group:\n",
    "\n",
    "* Maxime Dargent\n",
    "* Alessandra Chappuis\n",
    "* Julien Moreno\n",
    "* Lucas Pallez\n",
    "\n",
    "## Section 1: Introduction\n",
    "\n",
    "The goal of this project is to combine computer vision, global and local navigation, motion control and Kalman filtering to control a Thymio robot so that it reaches an arbitrarly placed goal in the environment. Our implementation consists in taking a first frame with the camera, convert it to an occupancy grid, compute the shortest path using the Djikstra algorithm, compute and send the instruction to the Thymio's motors so that it follows the path and continuosly check whether there is an unexpected obstacle that needs to be avoided.\n",
    "\n",
    "### Section 1.1: Libraries imports\n",
    "\n",
    "To run the code of our project, we need multiple imports that are shown in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### \n",
    "# The lines below are meant to install packages if some are missing\n",
    "#######\n",
    "\n",
    "# !pip install tqdm scipy\n",
    "# !pip install matplotlib\n",
    "# !pip install opencv-contrib-python\n",
    "# !pip install numpy\n",
    "# !pip install --upgrade tdmclient\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import time\n",
    "import numpy as np\n",
    "#from tqdm import tqdm\n",
    "#from kalmanfilter import KalmanFilter\n",
    "import cv2\n",
    "#from Motion_control import MotionControl\n",
    "#import local_nav as ln\n",
    "#import math \n",
    "from computer_vision import Vision\n",
    "#from djikstra import djikstra_algo\n",
    "#from calibration import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Connecting the Thymio robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmclient import ClientAsync, aw\n",
    "client = ClientAsync()\n",
    "node = aw(client.wait_for_node())\n",
    "aw(node.lock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Calibration \n",
    "\n",
    "In this section, we will run multiple functions, in order to calibrate our thymio to the current environement. This gives us more flexibility, and makes it easier to use different thymio in different environments.\n",
    "\n",
    "First, we need to initialize the calibration class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = 0.1\n",
    "SPEED_L = 101\n",
    "SPEED_R = 98\n",
    "GND_THRESHOLD = 400\n",
    "\n",
    "mc = MotionControl(node, client, Ts, SPEED_R, SPEED_L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Calibration of the Thymio \n",
    "\n",
    "In this first step, we will use the following image to calibrate the speed, its conversion ratio from the thymio's sensor to mm/s, and its variance for the Kalman filter.\n",
    "\n",
    "<img src=\"Pictures/Picture1.jpg\" width=\"900\"/>\n",
    "\n",
    "In this next function, we will ask the thymio to move forward, at a known speed. As soon as its ground sensor detect it is on the black line, it will start a timer. When it exits the line, the timer will stop, the thymio will stop aswell, and all the desired values will be computed. In this function, the line length is know and we need to be carefully align the thymio as to make it follow the line. From this test, we can also adjust the speed on the left and right wheel to make it go as straight as possible and adjust the transition threshold if the light's intensity in the current room requires it.\n",
    "\n",
    "The following cell needs to be executed in order to initialize the calibration class, and start the calibration process. It will then give back the values in the form of a print, and give two functions we can replace the ones mentionned earlier. We have made it this way for debugging purposes. When changing code in other files that are imported, we need to restart the jupyter kernel and rerun all the cells to initialize everything. But we already know the values of the previous calibration, and if we are in the same room, with the same map and the same thymio, we know these value shouldn't change by a large margin. The two new functions will initialize everything, paste the values found earlier, and won't run the calibration process again. And if we need to change it back, the two  new functions, for the calibration process, would again be provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data = data(Ts, SPEED_L, SPEED_R, GND_THRESHOLD, client, node, 0.329506587331065, 6.793596512574189)\n",
    "cal_data.calibration_mm(mc)\n",
    "mc.speed_conversion = cal_data.speed_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Calibration of the camera\n",
    "In this next cell, we will retrieve the the grid from the camera, and do all the conversions requiered for the rest of the project. \n",
    "\n",
    "We initialize the vision class here, with a calibration function, because of the light's density in our current room. We often needed to adjust the threshold for multiple detection process in this class. We could've automated the process, but have decided against it, as it never was a time-consuming task. We would usually run the cell bellow with the DEBUG defined as true, check the number of obstacles, and from this, we immediatly knew what to do with our threshold value, and change it once, or twice at the very maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful that if you have not called the previous cell, this will automatically set the missing values\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "nbAruco = 2\n",
    "threshold = 100\n",
    "calibrate = False\n",
    "vision, Q_cam, R_cam = cal_data.cam_calibration(calibrate, nbAruco, threshold)\n",
    "\n",
    "HALF_CELL_WIDTH = vision.cell_width/2\n",
    "if(DEBUG == True):\n",
    "    print(\"Number of obstacles = \", np.count_nonzero(vision.grid))\n",
    "    print(vision.grid)\n",
    "    lines = vision.show()\n",
    "    plt.figure\n",
    "    plt.title('Lines')\n",
    "    plt.imshow(cv2.cvtColor(lines.astype('uint8'), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: The environment\n",
    "We decided to discretize the environment in which the Thymio robot moves. Therefore, we created a 8x9 grid, in which all cells have the same dimension (145x145mm). Two types of obstacles can be present in the environment:\n",
    "* Static obstacles: these are black squared boxes that more or less fill up the whole cell they are in, they are present in the environment since the beginning and do not move. \n",
    "* Unexpected obstacles: these can be the same as the fixed obstacles or smaller, they show up in the environment unexpectedly and are avoided thanks to local navigation.\n",
    "\n",
    "We decided to only have 3D obstacles, as it simplifies the step after the local avoidance where the Thymio robot has to find back its path.\n",
    "\n",
    "Due to the discretization of the environement, we also decided to limit the motion of the robot to the four directions parallel to the x-y axis of the environment. Meaning that the motion of our robot will be characterized by 90 degrees turns and straight line motion. \n",
    "\n",
    "Both the Thymio robot and the goal are marked with ArUco markers, the bigger one corresponds to the goal and the other one is on top of the Thymio.\n",
    "\n",
    "The camera is placed above the environment, so that the distortion of the image is minimal.\n",
    "\n",
    "\n",
    "![Alt text](Pictures/img_environment.jpg)\n",
    "<center>Figure 2: Picture taken by the camera and reprensenting our environment with the Thymio robot and the goal.</center>\n",
    "    \n",
    "\n",
    "\n",
    "## Section 4: Computer Vision\n",
    "\n",
    "The computer vision module is represented by the class Vision in the *computer_vision.py* file. This class allows to process and store the images, to compute the goal position and some features about the robot's position (see the attributes of the class). We used the *OpenCV* library for most of the computations in this class.\n",
    "\n",
    "### Section 4.1: Computing the occupancy grid\n",
    "\n",
    "The first step is to compute the occupancy grid corresponding to the environment, the Thymio robot and the goal should not be in the environment yet. This is done only once at the beginning by creating an instance of the Vision class: \n",
    "\n",
    "```vision = Vision()```\n",
    "\n",
    "Which involves the following steps:\n",
    "\n",
    "**1) Taking a picture with the camera**\n",
    "\n",
    "**2) Processing the image**\n",
    "\n",
    "The goal of this function is to output an image where the changes of colors, and thus the edges, can be easily identified. The adaptive threshold function allows us to do that after blurring and converting the image to B&W. This processing is enough for the next steps and does not need more processing in order to highlight the edges in our camera. \n",
    "\n",
    "\n",
    "**3) Creating and applying a mask**\n",
    "\n",
    "To identify the environment in our image, we simply compute the contours thanks to the ```cv2.findContours()``` function and look for the contour having the biggest area. Then create a mask where the background of our image is black, while the part corresponding to the grid is white. [Source](https://maker.pro/raspberry-pi/tutorial/grid-detection-with-opencv-on-raspberry-pi)\n",
    "\n",
    "**4) Creating a transform to resize the image**\n",
    "\n",
    "The goal of this step is to resize the image so that ```vision.img_final``` only contains the part of the image that corresponds to the grid. To do so, we need to compute a transform that maps the edges of the environment to the smallest fitting rectangle. \n",
    "The first step is the computation of the contour with the biggest area, corresponding to the environment. To reduce the noise and thus the errors, we use the mask computed previously. Then, as suggested by this [source](https://stackoverflow.com/questions/5838506/opencv-find-skewed-rectangle), we compute the convex hull of that contour. Thanks to this [github code](https://gist.github.com/arccoder/7b98761478aace53899cd22448b80c71) we are able to compute the smallest fitting rectangle and sort the edges clockwise.\n",
    "\n",
    "**5) Computing the occupancy grid**\n",
    "\n",
    "To compute the occupancy grid, we need to know the number of rows and columns in the grid. To simplify the computation, we decided to give to the program those numbers. The number 1 in the grid corresponds to an obstacle, while 0 is free-space.\n",
    "\n",
    "There is a variable ```vision.isCamOn``` that represents whether the camera was able to take a picture. If not, then the coordinates of the goal and the robot cannot be computed.\n",
    "\n",
    "\n",
    "Below are shown the images obtained at each step if we use the image shown above and the corresponding grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR MESSAGE: \u001b[00mcould not open the video stream.\n",
      "\u001b[31mERROR MESSAGE: \u001b[00m cannot receive frame.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5d3005792821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvision_ex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m### equivalent to vision.occupancy_grid() that is called in the initialisation of an instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvision_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pictures/img_nothymio.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvision_ex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1- EPFL/MA1/Mobile Robotics/FinalProject/computer_vision.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, threshold)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moccupancy_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtake_picture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1- EPFL/MA1/Mobile Robotics/FinalProject/computer_vision.py\u001b[0m in \u001b[0;36moccupancy_grid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mCall\u001b[0m \u001b[0mit\u001b[0m \u001b[0monly\u001b[0m \u001b[0monce\u001b[0m \u001b[0mper\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_picture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misCamOn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/1- EPFL/MA1/Mobile Robotics/FinalProject/computer_vision.py\u001b[0m in \u001b[0;36mtake_picture\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input/frame.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/imgcodecs/src/loadsave.cpp:801: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
     ]
    }
   ],
   "source": [
    "vision_ex = Vision()\n",
    "\n",
    "### equivalent to vision.occupancy_grid() that is called in the initialisation of an instance\n",
    "vision_ex.img = cv2.imread('Pictures/img_nothymio.png', cv2.IMREAD_COLOR)\n",
    "vision_ex.image_processing()\n",
    "vision_ex.create_mask()\n",
    "vision_ex.apply_mask()\n",
    "vision_ex.create_transform()\n",
    "vision_ex.apply_transform()\n",
    "vision_ex.create_grid()\n",
    "vision_ex.conversion_factor_x = (vision_ex.cell_width)/vision_ex.cellx\n",
    "vision_ex.conversion_factor_y = (vision_ex.cell_width)/vision_ex.celly\n",
    "###\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "rows = 2\n",
    "columns = 2\n",
    "\n",
    "img1 = cv2.imread('Pictures/img_nothymio.png')\n",
    "img2 = cv2.imread('intermediate/processedImage.png')\n",
    "img3 = cv2.imread('intermediate/maskOnImage.png')\n",
    "img4 = cv2.imread('output/transformedImage.png')\n",
    "\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img1.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(\"Original image\")\n",
    "\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img2.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(\"Processed image\")\n",
    "\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img3.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(\"Mask applied on the image\")\n",
    "\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img4.astype('uint8'), cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.title(\"Final image: resized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 3: Images created at each step of the initialization of the class Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The occupancy grid corresponding to the environment is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Occupancy grid: \\n\", vision_ex.grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see in the pictures above, as well as in the code, we add an offset in the ```vision.create_transform()``` function, so that we have a border of background around the environment. This is done to avoid the case where the robot moves too close to the boundaries of the environment and its ArUco marker is cut off by the resizing of the image (due to the distortion). By adding an offset, we take into considerantion the environment and a bit of background around it, so that the ArUco marker can be seen even if the robot is close to the edges.\n",
    "\n",
    "\n",
    "### Section 4.2: Compute the Thymio and goal coordinates\n",
    "\n",
    "Once we computed the occupancy grid, both the Thymio robot and the goal can be added to the environment, and their coordinates can be calculated by calling the ```vision.update_coordinates()``` function. This call can be done as many times as needed.\n",
    "\n",
    "In order to localize the Thymio robot and the goal, we used 4X4 ArUco markers and the ArUco library functions. Indeed, thanks to the coordinates of the four corners, we are able to estimate the pose and orientation of that marker. The choice of using ArUco markers was determined by its simplicity and its lower dependence on the lightning conditions. Indeed, we first though about using two colored circle to localize the Thymio robot and calculate its orientation, but that setup was depending a lot on the lightning and thus needed a lot of image processing to minimize that effect. To process the image before detecting the ArUco markers, we only need to convert it to B&W and apply a binary threshold that will separate the markers from the background. This threshold is set depending on the lightning of the room we are in: the more light, the smaller the threshold. \n",
    "\n",
    "Sometimes the ArUco markers are not detected by the camera, and thus the coordinates cannot be updated. There are multiple reasons why the markers are not detected:\n",
    "* the lightning conditions changed\n",
    "* an obstacle obstructs the vision of the marker on the Thymio\n",
    "* the Thymio moves too close or out of to the boundaries of the environment\n",
    "* ...\n",
    "\n",
    "In those cases, the other modules need to know that they cannot use the coordinates computed by the camera. This is done by checking whether ```vision.thymio``` and ```vision.goal``` are set to ```True```. If not, this means that the corresponding marker was not detected. When a frame could not be taken, both the variables are automatically set to ```False```.\n",
    "\n",
    "Below is the code that runs inside ```vision.update_coordinates()```. We show below an image with the estimated pose of the Thymio robot (red) and the goal (green). There is a small deviation, probably due to the many approximations we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### equivalent to update_coordiantes()\n",
    "vision_ex.img = cv2.imread('Pictures/img_thymio.png', cv2.IMREAD_COLOR)\n",
    "vision_ex.apply_mask()\n",
    "vision_ex.apply_transform()\n",
    "vision_ex.coordinates()\n",
    "###\n",
    "\n",
    "print(\"The orientation of the Thymio robot is: \", vision_ex.thymio_orientation)\n",
    "\n",
    "img = cv2.imread('output/transformedImage.png').copy()\n",
    "pos_x = round(vision_ex.offset+(vision_ex.thymio_position[0] + 0.5)*vision_ex.cellx)\n",
    "pos_y = round(vision_ex.offset+(vision_ex.thymio_position[1] + 0.5)*vision_ex.celly)\n",
    "cv2.circle(img, (pos_x, pos_y), 5, (0,0,255), 6)\n",
    "\n",
    "pos_x = round(vision_ex.offset+(vision_ex.goal_position[0] + 0.5)*vision_ex.cellx)\n",
    "pos_y = round(vision_ex.offset+(vision_ex.goal_position[1] + 0.5)*vision_ex.celly)\n",
    "cv2.circle(img, (pos_x, pos_y), 5, (0,255,0), 6)\n",
    "\n",
    "plt.figure\n",
    "plt.title(\"Pose estimation of the robot and the goal\")\n",
    "plt.set()\n",
    "plt.imshow(cv2.cvtColor(img.astype('uint8'), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 4: Estimated pose of the robot (red) and goal (green) by the camera.\n",
    "\n",
    "This class also allows to plot the expected path and the executes path followed by the Thymio robot. Indeed, there is a function called ```vision.create_path```, that takes the ```shortest_path``` as an argument and returns an image that shows both paths: the Dijkstra path (red) and the executed one (red).\n",
    "\n",
    "![Alt text](Pictures/path_compare.png)\n",
    "Figure 5: Comparison of the paths for a given situation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Global Navigation\n",
    "\n",
    "For the global navigation, we decided to use the Djikstra algorithm, as it is the most efficient and simple way to plan a global path in a grid.\n",
    "To implement it, we used the Serie 5 of the Mobile Robotics course, and adapted it to fit to our environment : the first task was to change the grid from a square to a rectangle, adapting to any size and shape of grid. This was done by creating a ```max_val_x``` and ```max_val_y``` instead of the ```max_val``` previously implemented, and adapt it in the plot creation. \n",
    "\n",
    "The second task was to adapt the grid so that the points would appear in the middle of the cells, and not on intersections like in the Serie 5. That was implemented in the function ```create_empty_plot``` of *djikstra.py* : we used major ticks as the true coordinates of our path, and hid the lines. We then set the minor ticks to coord-0.5 in order to have the cells being drawn. We also had to invert the y axis, as in computer vision, a picture has the origin on the top left of the frame, whereas in the grid previously implemented, the origin was on the bottom left of the frame. The improvements are shown in Fig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "img1=open('Pictures/djikstrabad4N.png','rb').read()\n",
    "wi1 = widgets.Image(value=img1, format='png', width=350, height=500)\n",
    "img2=open('Pictures/djikstragood4N.png','rb').read()\n",
    "wi2 = widgets.Image(value=img2, format='png', width=350, height=500)\n",
    "a=[wi1,wi2]\n",
    "wid=widgets.HBox(a)\n",
    "display.display(wid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">Figure 5 : Shortest path with Djikstra algorithm showed on a rectangular grid with the position of the thymio on the intersection(left) and in the center of each cell (right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another task was to block certain movements : our first iteration of the djikstra algorithm worked for the '8N' movements. In that case, the algorithm theoretically passes through in every diagonal if the cell is free. In reality, the robot doesn't have the space to go through. We implemented a condition in the Astar function to be able to prevent this situation.\n",
    "```\n",
    "if deltacost == math.sqrt(2) :\n",
    "   if (occupancy_grid[neighbor[0]-np.sign(dx), neighbor[1]]) or (occupancy_grid[neighbor[0], neighbor[1]-np.sign(dy)]): \n",
    "      continue\n",
    "```\n",
    "**This condition wasn't implemented in the final project, because the group decided to go with 4N movements to make kalman filtering easier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Pictures/djikstra8N.png)\n",
    "<div align=\"center\">Figure 6 : Shortest path with Djikstra algorithm on a grid with the 8N type movements showing the condition to prevent imppossible movements in diagonal on coordinates (8,13)->(9,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Local Navigation\n",
    "\n",
    "For the local navigation module, we decided to go with a simple yet efficient method : we proceed to follow the obstacle until the path is free again. The top level implementation is shown in Fig. We used a finite state machine to describe the local avoidance process. \n",
    "\n",
    "![Alt text](Pictures/local_fsm.png)\n",
    "\n",
    "Figure 7 : Finite State Machine of the local navigation : the state of the thymio robot can switch from HORIZONTAL_MOVE to VERTICAL_MOVE to OBST_TO_PATH and inversly\n",
    "\n",
    "\n",
    "When an obstacle is detected (the function obstacle_detect returns true) the robot will turn right to try to get around the obstacle. At this point the robot will be in the **HORIZONTAL_MOVE** state. If an obstacle is detected on the right, a variable called going_left is set to '1' and the robot tries to get around the obstacle from the left.\n",
    "\n",
    "![Alt text](Pictures/local_left.png) \n",
    "<center>Figure 8 : simulation scenario of the local navigation : if the robot is blocked on the right and in the front, it will activate a boolean going_left and avoid the obstacle from the left</center>\n",
    "\n",
    "\n",
    "In both cases, the robot will continue going forward until the sensors on the side (0 if the robot is turning right and 4 if it is turning left) do not detect the obstacle anymore, and a formula integrated in the function will make the thymio go one case further to stay in the center of the cell and make sure it has space to turn. If the thymio has successfully passed the obstacle honrizontally, it switches to **VERTICAL_MOVE**. In that state, the robot will try to get around the obstacle vertically. The robot will continue going forward until either the sensors on the side do not detect the obstacle anymore or if there is an obstacle in the front. In the last case, it will switch back to **HORIZONTAL_MOVE** state and go further away from the obstacle to get around it.\n",
    "\n",
    "![Alt text](Pictures/local_HVHV.png) \n",
    "<center>Figure 9 : simulation scenario of the local navigation : if the robot sees an obstacle while being in the vertical move state, it can go back to the horizontal state to get around it\n",
    "</center>\n",
    "\n",
    "\n",
    "If there is not obstacle at the front, it can switch to the **OBST_TO_PATH** state. In this state, the robot will try to go back to the point he was supposed to be after the obstacle.\n",
    "\n",
    "![Alt text](Pictures/local_normal.png) \n",
    "<center>Figure 10 : simulation scenario of the local navigation : typical run of the local avoidance function, moving from the HORIZONTAL_MOVE to the VERTICAL_MOVE to the OBST_PATH_MOVE state directly.\n",
    "</center>\n",
    "\n",
    "\n",
    "However, this behavious is not optimized, because the robot could in that case go through a cell that is part of the path and still go back to the cell and then come back to the same cell. We then decided to include the shortest path as a parameter of the function obstacle_avoid, to be able to check if the coordinates of the robot is part of the path. In that case the robot will switch back to the main master function, sending it's coordinates and orientation.  \n",
    "\n",
    "![Alt text](Pictures/local_path.png)\n",
    "<center>Figure 11 : simulation scenario of the local navigation : if the robot crosses the path without being at the goal, it still joins the path\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Kalman Filtering\n",
    "\n",
    "The filter we have chosen to use is the kalman's filter. With the use of the camera, and the thymio's speed sensor, we have two different sensor with which we can use the Kalman's filter. It's implementation is traditionnal. Again, we have gone for the usage of a class, as it makes it easier to use, and track the values. \n",
    "The one section where we had room to play with, were the values of the covariance of process noise's matrices Q, and the values of the covariance of the measurement's noise R.\n",
    "\n",
    " $R = \\begin{bmatrix} r_p & 0\\\\ 0 & r_\\nu \\end{bmatrix}$\n",
    " $Q = \\begin{bmatrix} q_p & 0\\\\ 0 & q_\\nu \\end{bmatrix}$\n",
    "\n",
    "The values have been computed using the same method as in the exercise of week 8. The $r_\\nu$ and $q_\\nu$ have been computed inside the calibration process. For $q_p$ and $r_p$, it was a little bit more difficult. An issue we had, was the deformation of the image due to the lens, and the position of the camera. \n",
    "\n",
    "<img src=\"Pictures/Camera_pos.jpg\" />\n",
    "\n",
    "When placing the camera, we always placed it perfectly in the middle of the grid along its x axis, but because of our setup, we could never place it in the middle of its width along the y axis aswell. It was always on the side. This induced an error on the y position of the thymio. We have fixed it by placing a y_offset in the vision class. With this offset, we managed to correct the position in y from the camera's perspective, but it wasn't perfect either. We still had an error of approximatively ±5mm. Based on the exercise of week 8 again, we decided to set $r_p$ = 0.01 But while testing this, we also managed to see the variance on the position from the camera's perspective. We found from this that the standard deviation was ~0.5mm, so we have set $q_p$ =  0.25mm\n",
    "\n",
    "\n",
    "For this filter, we've inspired ourselves from the courses and the youtuber \"L42\" and his [github](https://github.com/L42Project/Tutoriels/tree/master/Divers/tutoriel36), to properly understand the filter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Motion control\n",
    "The motion control module regroups every functions related to the thymio's various movements. In there is located the motors function which makes the thymio move according to the left and right speed. Other functions are used to estimate the next trajectory of the robot : get_turn and correct_orientation: get_turn computes the movement needed for the thymio to get to the orientation desired according to the next coordinates in the Djikstra algorithm and the actual orientation. To prevent useless turns, we substract or add 4 to the movement in case it is superior to 2. That means that is the robot wants to turn 270°, it will turn -90° instead. correct_orientation is usefull to make sure that even if the camera measures an angle that is not exactly a multiple of 90°, the orientation set for the thymio is still correct, taking into account an error in the measure.\n",
    "\n",
    "A function has been implemented to make the robot turn : indeed, the center of rotation of the thymio isn't located at it's center; when we try to have both motor running at opposite speeds, that causes the robot's center after a turn to be far from the center of the cell. We decided to implement a perpendicular parking like behaviour, calibrating the values to be able to fit in the cells. \n",
    "\n",
    "![Alt text](Pictures/turn.png)\n",
    "<center>Figure 13 : simulation scenario of the movement of the thymio when it does a 90° turn in the left direction : it is done in 3 steps\n",
    "</center>\n",
    "\n",
    "### Section 8.1: Kalman_adjust\n",
    "\n",
    "With the kalman filter, we get an estimated position of our thymio, from our multiple sensor. But we still need to use these informations. Because of our environement and path finding choices, we have decided to run the Kalman_adjust per cell. This means that we will run the correction *per steps*. We take the center of our thymio from the kalman's estimated position, and take the difference with the center of the next targeted cell. The goal is for the thymio to face the center of this cell. This is what Kalman_adjust do. It computes the angle required to reach the center of the next targeted cell, compares it with the actual thymio's orientation, and runs a proportionnal controller to fix the error. We found two approaches: in both case, we increase the speed of one wheel, while we decrease the speed of the other. This will lead to a rotation of the thymio. Then we could either decide to let this different in speed be, and expect kalman to correct the speed at every sampling time, or, the second approach, the one we went with, set this difference in speed, and let it run for a little time, then reset the speed back to normal. We have decided to set the duration to two sampling time, to be sure it would be long enough for it to affect the locomotion. The first option has also been tested, but resulted in a lot of oscillation from the thymio.\n",
    "\n",
    "The gain was set experimentally. We know that the $k_p * \\omega * t$  ($\\omega$ being the angular velocity) shouldn't exceed the difference in angle from the desired orientation and the actual one. We could've increased the gain up to this limit but found the value of 2.5 was good enough, without being too agressive.\n",
    "Finally, we have also made the choice of ignoring a difference in angle bellow our threshold of 7°, because of this imperfection of our setup, and the camera's deformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Master interface\n",
    "The master interface is a function which regroups every module together to make the robot go from start to goal. The implementation is the following : from the computer vision's occupancy grid, we retrieve the shortest path with the djiksta algorithm. We then start a loop taking each dx and dy in shortest path, dx and dy being the absolute index in the grid. At each iteration, we update the camera and correct the angle of the thymio. Then, if neccessary, the thymio turns according to the new coordinates desired. It then checks if there is any obstacle in front of him before going forward. If that's the case, it stays in the local loop while the thymio hasn't passed around the obstable. Then the master loop turns empty thanks to the jump variable, until the neew coordinates of the thymio match the global path. \n",
    "\n",
    "If there is no obstacle in front of him, the thymio won't need to go in local navigation mode, and can move freely forward, with a distance of one cell. During this movement, the kalman filtering is applied to make sure that the robot stays in the right direction. When the movement is done, the coordinates x and y actualize with the value of dx and dy. \n",
    "\n",
    "If the goal is moved during the process, a check is done every step; if that is the case, we break from the loop of the shortest path to recalculate the new path with djikstra, as well as reinitializing the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mc.step_duration = HALF_CELL_WIDTH*2 / (mc.SPEED_AVG * mc.speed_conversion)\n",
    "mc.turn_duration = 98 / (mc.SPEED_AVG * mc.speed_conversion)\n",
    "restart = True\n",
    "\n",
    "jump_x, jump_y = 0,0\n",
    "MARGIN = 40\n",
    "change_dir = False\n",
    "while (restart == True):\n",
    "    jump = False\n",
    "    index = 0\n",
    "    restart = False\n",
    "    vision.update_coordinates()\n",
    "    if(DEBUG == True):\n",
    "        print(\"thymio real pos : \", vision.thymio_real_pos)\n",
    "        print(\"thymio pos in grid : \", vision.thymio_position)\n",
    "        print(\"goal pos : \", vision.goal_position)\n",
    "        print(\"thymio angle = :\", vision.thymio_orientation)\n",
    "    vision.grid[vision.thymio_position[1]][vision.thymio_position[0]] = 0\n",
    "    vision.grid[vision.goal_position[1]][vision.goal_position[0]] = 0\n",
    "    shortest_path = djikstra_algo(vision.grid.T, vision.thymio_position, vision.goal_position)\n",
    "    if(DEBUG == True):\n",
    "        print(shortest_path)\n",
    "    KF = KalmanFilter(Ts, vision.thymio_real_pos, cal_data.speed_conversion, Q_cam, R_cam)\n",
    "    mc.orientation = mc.correct_orientation(vision.thymio_orientation)\n",
    "    x = vision.thymio_position[0]\n",
    "    y = vision.thymio_position[1]\n",
    "    speed = np.array([SPEED_L, SPEED_R])\n",
    "    turn_speed = np.array([0, 0])\n",
    "    for dx,dy in np.transpose(shortest_path):\n",
    "        if jump:\n",
    "            x = dx      #actualize the coordinates of the robot\n",
    "            y = dy      #actualize the coordinates of the robot\n",
    "            index += 1\n",
    "            if jump_x == dx and jump_y == dy:\n",
    "                jump = False\n",
    "            continue  \n",
    "        vision.update_coordinates()\n",
    "        if(vision.goal_position != vision.goal_previous) & (change_dir != True):\n",
    "            change_dir = True\n",
    "            restart = True\n",
    "            break\n",
    "        turn = mc.get_turn(dx-x,dy-y,mc.orientation)\n",
    "        mc.orientation = (mc.orientation + turn)%4\n",
    "        for i in range(abs(turn)):\n",
    "            mc.robot_turn(np.sign(turn))\n",
    "        mc.adjust_angle(vision)\n",
    "        if (((dx-x)!=0) | ((dy-y)!=0)):\n",
    "            local = ln.obstacle_detect(node)\n",
    "            if local:\n",
    "                if(DEBUG == True):\n",
    "                    print(\"obstacle\",len(shortest_path[1]))\n",
    "                jump,jump_x,jump_y = ln.obstacle_avoid(vision, mc, x, y, shortest_path, index, node, client)\n",
    "            else:   \n",
    "                if (((dx-x)!=0) | ((dy-y)!=0)):\n",
    "                    step_done = False\n",
    "                    start_move = time.time()\n",
    "                    mc.motors(speed[0], speed[1])\n",
    "                    temp = 0\n",
    "                    next_target_x = dx *HALF_CELL_WIDTH*2 + HALF_CELL_WIDTH\n",
    "                    next_target_y = (vision.rows - 1 - dy) *HALF_CELL_WIDTH*2 + HALF_CELL_WIDTH\n",
    "                    \n",
    "                    while (step_done != True):  \n",
    "                        vision.update_coordinates()\n",
    "                        kalman_pos= KF.filter(vision.thymio, vision.thymio_real_pos, speed, vision.thymio_orientation)\n",
    "                        if(DEBUG == True):\n",
    "                            print(\"estimated position \", kalman_pos)\n",
    "                            print(\"position from camera \", vision.thymio_real_pos)\n",
    "                        delta_x, delta_y= mc.kalman_adjust(next_target_x, next_target_y, kalman_pos, vision.thymio_orientation)\n",
    "                        current = time.time()\n",
    "                        temp = current - start_move\n",
    "                        if((np.abs(delta_x) < MARGIN) & (np.abs(delta_y) < MARGIN)):\n",
    "                            step_done = True\n",
    "                            mc.motors(0, 0) \n",
    "                        elif(temp > mc.step_duration):\n",
    "                            step_done = True  \n",
    "                            mc.motors(0, 0)\n",
    "                \n",
    "                    mc.adjust_angle(vision)\n",
    "\n",
    "        x = dx      #actualize the coordinates of the robot\n",
    "        y = dy      #actualize the coordinates of the robot\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: Conclusion\n",
    "\n",
    "This project being made of random groups, and over a very short time, forced us to cooperate and communicate properly. We managed to work all together, and help each other face problem we all faced. The task was certainly not always easy, but having this cooperation inside the group was a key aspect of the work we have done.\n",
    "\n",
    "We've also learned, with a practical point of view, how path finding, local navigation, image processing and the filtering actually works\n",
    "\n",
    "We have set a defined goal for this project and feel we have managed to succeed. Surely, we could've made some aspect of our project more raffined, ellaborate, and/or versatile, but given the time available for this project, shared with other project with other group, we are happy with the results.\n",
    "\n",
    "## Section 11: Demonstration\n",
    "\n",
    "You can find a demonstration run of our project at the following link : https://drive.google.com/drive/folders/1M95SnwzH_0C-xljhxi9llSEg_ssDAt28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ff010c4fb58d66583690da57b105aade205a845f835d32c4ee88b21f1478465"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
